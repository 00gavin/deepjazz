![deepjazz](https://cloud.githubusercontent.com/assets/9053987/14231549/83f21b74-f955-11e5-9477-a999aa8d94b0.png)
### Using Keras & Theano for deep learning driven jazz generation
I built this project for HackPrinceton, Spring 2016. It uses Keras & Theano, two deep learning libraries, to generate jazz music. deepjazz learns from an input MIDI file and uses what it has learned to generate new jazz! It uses deep learning, the AI technology that powers [Google's AlphaGo](https://deepmind.com/alpha-go.html) and [IBM's Watson](https://www.ibm.com/smarterplanet/us/en/ibmwatson/what-is-watson.html), **to make music -- something that's considered as deeply human**.

### Want to listen?
Check out deepjazz's music on **[Soundcloud](https://soundcloud.com/deepjazz-ai)**!

### Author
[Ji-Sung Kim](https://www.linkedin.com/in/jisungkim)  
Princeton University, Department of Computer Science  
jisungk@princeton.edu  

### Citations
This project was inspired by and adapts a lot of preprocessing code (with permission) from Evan Chow's [jazzml](https://github.com/evancchow/jazzml). Thank you [Evan](https://www.linkedin.com/in/evancchow)! Public examples from the [Keras documentation](https://github.com/fchollet/keras) were also referenced.

### License
Apache License 2.0